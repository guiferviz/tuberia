{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Data engineering meets software engineering Documentation : https://guiferviz.github.io/tuberia Source Code : https://github.com/guiferviz/tuberia \ud83e\udd14 What is this? Tuberia is born from the need to bring the worlds of data and software engineering closer together. Here is a list of common problems in data projects: Loooooong SQL queries impossible to understand/test. A lot of duplicate code due to the difficulty of reusing it in SQL queries. Lack of tests, sometimes because the used framework does not facilitate testing tasks. Lack of documentation. Discrepancies between the existing documentation and the latest deployed code. A set of notebooks deployed under the Databricks Share folder. A generic notebook with utility functions. Use of drag-and-drop frameworks that limit the developer's creativity. Months of intense work to migrate existing pipelines from one orchestrator to another (e.g. from Airflow to Prefect, from Databricks Jobs to Data Factory...). Tuberia aims to solve all these problems and many others. \ud83e\udd13 How it works? You can view Tuberia as if it were a compiler. Instead of compiling a programming language, it compiles the steps necessary for your data pipeline to run successfully. Tuberia is not an orchestrator, but it allows you to run the code you write in Python in any existing orchestrator: Airflow, Prefect, Databricks Jobs, Data Factory.... Tuberia provides some abstraction of where the code is executed, but defines very well what are the necessary steps to execute it. For example, this shows how to create a PySpark DataFrame from the range function and creates a Delta table. import pyspark.sql.functions as F import tuberia as tb class Range ( tb . spark . Table ): \"\"\"Table with numbers from 1 to `n`. Attribute: n: Max number in table. \"\"\" n : int = 10 class schema : id = tb . column ( int ) def df ( self ): return self . spark . range ( self . n ) . withColumn ( self . schema . id , F . col ( \"id\" ), # This id comes from the range function, # so we don't use self.schema.id here ) class DoubleRange ( tb . spark . Table ): class schema : id = tb . column ( int ) range : Range = Range () def df ( self ): return self . range . read () . withColumn ( self . schema . id , F . col ( self . range . schema . id ) * 2 ) tb . run ( DoubleRange ()) Warning Previous code may not work yet and it can change. Please, notice this project is in an early stage of its development. All docstrings included in the code will be used to generate documentation about your data pipeline. That information, together with the result of data expectations/data quality rules will help you to always have complete and up to date documentation. Besides that, as you have seen, Tuberia is pure Python so doing unit tests/data tests is very easy. Programming gurus will enjoy data engineering again!","title":"Home"},{"location":"#what-is-this","text":"Tuberia is born from the need to bring the worlds of data and software engineering closer together. Here is a list of common problems in data projects: Loooooong SQL queries impossible to understand/test. A lot of duplicate code due to the difficulty of reusing it in SQL queries. Lack of tests, sometimes because the used framework does not facilitate testing tasks. Lack of documentation. Discrepancies between the existing documentation and the latest deployed code. A set of notebooks deployed under the Databricks Share folder. A generic notebook with utility functions. Use of drag-and-drop frameworks that limit the developer's creativity. Months of intense work to migrate existing pipelines from one orchestrator to another (e.g. from Airflow to Prefect, from Databricks Jobs to Data Factory...). Tuberia aims to solve all these problems and many others.","title":"\ud83e\udd14 What is this?"},{"location":"#how-it-works","text":"You can view Tuberia as if it were a compiler. Instead of compiling a programming language, it compiles the steps necessary for your data pipeline to run successfully. Tuberia is not an orchestrator, but it allows you to run the code you write in Python in any existing orchestrator: Airflow, Prefect, Databricks Jobs, Data Factory.... Tuberia provides some abstraction of where the code is executed, but defines very well what are the necessary steps to execute it. For example, this shows how to create a PySpark DataFrame from the range function and creates a Delta table. import pyspark.sql.functions as F import tuberia as tb class Range ( tb . spark . Table ): \"\"\"Table with numbers from 1 to `n`. Attribute: n: Max number in table. \"\"\" n : int = 10 class schema : id = tb . column ( int ) def df ( self ): return self . spark . range ( self . n ) . withColumn ( self . schema . id , F . col ( \"id\" ), # This id comes from the range function, # so we don't use self.schema.id here ) class DoubleRange ( tb . spark . Table ): class schema : id = tb . column ( int ) range : Range = Range () def df ( self ): return self . range . read () . withColumn ( self . schema . id , F . col ( self . range . schema . id ) * 2 ) tb . run ( DoubleRange ()) Warning Previous code may not work yet and it can change. Please, notice this project is in an early stage of its development. All docstrings included in the code will be used to generate documentation about your data pipeline. That information, together with the result of data expectations/data quality rules will help you to always have complete and up to date documentation. Besides that, as you have seen, Tuberia is pure Python so doing unit tests/data tests is very easy. Programming gurus will enjoy data engineering again!","title":"\ud83e\udd13 How it works?"},{"location":"changelog/","text":"Changelog Warning As this project is in an early stage of development, this file is not being used yet. In the future, all notable changes to this project will be documented in this file. The format is inspired on Keep a Changelog . This project uses Semantic Versioning . 0.0.1 - Unreleased 0.0.0 - 2022-06-19 Empty package.","title":"Changes"},{"location":"changelog/#changelog","text":"Warning As this project is in an early stage of development, this file is not being used yet. In the future, all notable changes to this project will be documented in this file. The format is inspired on Keep a Changelog . This project uses Semantic Versioning .","title":"Changelog"},{"location":"changelog/#001-unreleased","text":"","title":"0.0.1 - Unreleased"},{"location":"changelog/#000-2022-06-19","text":"Empty package.","title":"0.0.0 - 2022-06-19"},{"location":"contributing/","text":"Contributing to Tuberia Getting started You need: Spark 3.2. Java JDK 11 (Required by Spark). Poetry . Make. Once you have all the tools installed just open a shell on the root folder of the project and install the dependencies in a new virtual environment with: $ make install The previous command also installs some pre-commits . Check that your package is installed with: $ poetry run tuberia \u2584\u2584\u2584\u2588\u2588\u2588\u2588\u2588\u2593 \u2588 \u2588\u2588 \u2584\u2584\u2584\u2584 \u2593\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2580\u2588\u2588\u2588 \u2588\u2588\u2593 \u2584\u2584\u2584 \u2593 \u2588\u2588\u2592 \u2593\u2592 \u2588\u2588 \u2593\u2588\u2588\u2592\u2593\u2588\u2588\u2588\u2588\u2588\u2584 \u2593\u2588 \u2580 \u2593\u2588\u2588 \u2592 \u2588\u2588\u2592\u2593\u2588\u2588\u2592\u2592\u2588\u2588\u2588\u2588\u2584 \u2592 \u2593\u2588\u2588\u2591 \u2592\u2591\u2593\u2588\u2588 \u2592\u2588\u2588\u2591\u2592\u2588\u2588\u2592 \u2584\u2588\u2588\u2592\u2588\u2588\u2588 \u2593\u2588\u2588 \u2591\u2584\u2588 \u2592\u2592\u2588\u2588\u2592\u2592\u2588\u2588 \u2580\u2588\u2584 \u2591 \u2593\u2588\u2588\u2593 \u2591 \u2593\u2593\u2588 \u2591\u2588\u2588\u2591\u2592\u2588\u2588\u2591\u2588\u2580 \u2592\u2593\u2588 \u2584 \u2592\u2588\u2588\u2580\u2580\u2588\u2584 \u2591\u2588\u2588\u2591\u2591\u2588\u2588\u2584\u2584\u2584\u2584\u2588\u2588 \u2592\u2588\u2588\u2592 \u2591 \u2592\u2592\u2588\u2588\u2588\u2588\u2588\u2593 \u2591\u2593\u2588 \u2580\u2588\u2593\u2591\u2592\u2588\u2588\u2588\u2588\u2592\u2591\u2588\u2588\u2593 \u2592\u2588\u2588\u2592\u2591\u2588\u2588\u2591 \u2593\u2588 \u2593\u2588\u2588\u2592 \u2592 \u2591\u2591 \u2591\u2592\u2593\u2592 \u2592 \u2592 \u2591\u2592\u2593\u2588\u2588\u2588\u2580\u2592\u2591\u2591 \u2592\u2591 \u2591\u2591 \u2592\u2593 \u2591\u2592\u2593\u2591\u2591\u2593 \u2592\u2592 \u2593\u2592\u2588\u2591 \u2591 \u2591\u2591\u2592\u2591 \u2591 \u2591 \u2592\u2591\u2592 \u2591 \u2591 \u2591 \u2591 \u2591\u2592 \u2591 \u2592\u2591 \u2592 \u2591 \u2592 \u2592\u2592 \u2591 \u2591 \u2591\u2591\u2591 \u2591 \u2591 \u2591 \u2591 \u2591 \u2591\u2591 \u2591 \u2592 \u2591 \u2591 \u2592 \u2591 \u2591 \u2591 \u2591 \u2591 \u2591 \u2591 \u2591 Version 0 .0.0 If you can see that funky logo your installation is correct. Note that the version may change. If you do not want to use poetry run in front of all your commands just activate the virtual environment with poetry shell . Use exit if you want to deactivate the environment. How do I build the package? You can build the package without installing the dependencies or without a proper Spark installation. Use make build or just make . You should see something like: $ make poetry build Building tuberia ( 0 .0.0 ) - Building sdist - Built tuberia-0.0.0.tar.gz - Building wheel - Built tuberia-0.0.0-py3-none-any.whl How do I run tests? Run tests locally with: $ make test Contribution guidelines The code is auto-formatted by Black, so you can write the code without following any style guide and a Black pre-commit will take care of making it consistent with the current codebase. Write tests: test not added in the PR, test that will never be added.","title":"Contributing"},{"location":"contributing/#contributing-to-tuberia","text":"","title":"Contributing to Tuberia"},{"location":"contributing/#getting-started","text":"You need: Spark 3.2. Java JDK 11 (Required by Spark). Poetry . Make. Once you have all the tools installed just open a shell on the root folder of the project and install the dependencies in a new virtual environment with: $ make install The previous command also installs some pre-commits . Check that your package is installed with: $ poetry run tuberia \u2584\u2584\u2584\u2588\u2588\u2588\u2588\u2588\u2593 \u2588 \u2588\u2588 \u2584\u2584\u2584\u2584 \u2593\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2580\u2588\u2588\u2588 \u2588\u2588\u2593 \u2584\u2584\u2584 \u2593 \u2588\u2588\u2592 \u2593\u2592 \u2588\u2588 \u2593\u2588\u2588\u2592\u2593\u2588\u2588\u2588\u2588\u2588\u2584 \u2593\u2588 \u2580 \u2593\u2588\u2588 \u2592 \u2588\u2588\u2592\u2593\u2588\u2588\u2592\u2592\u2588\u2588\u2588\u2588\u2584 \u2592 \u2593\u2588\u2588\u2591 \u2592\u2591\u2593\u2588\u2588 \u2592\u2588\u2588\u2591\u2592\u2588\u2588\u2592 \u2584\u2588\u2588\u2592\u2588\u2588\u2588 \u2593\u2588\u2588 \u2591\u2584\u2588 \u2592\u2592\u2588\u2588\u2592\u2592\u2588\u2588 \u2580\u2588\u2584 \u2591 \u2593\u2588\u2588\u2593 \u2591 \u2593\u2593\u2588 \u2591\u2588\u2588\u2591\u2592\u2588\u2588\u2591\u2588\u2580 \u2592\u2593\u2588 \u2584 \u2592\u2588\u2588\u2580\u2580\u2588\u2584 \u2591\u2588\u2588\u2591\u2591\u2588\u2588\u2584\u2584\u2584\u2584\u2588\u2588 \u2592\u2588\u2588\u2592 \u2591 \u2592\u2592\u2588\u2588\u2588\u2588\u2588\u2593 \u2591\u2593\u2588 \u2580\u2588\u2593\u2591\u2592\u2588\u2588\u2588\u2588\u2592\u2591\u2588\u2588\u2593 \u2592\u2588\u2588\u2592\u2591\u2588\u2588\u2591 \u2593\u2588 \u2593\u2588\u2588\u2592 \u2592 \u2591\u2591 \u2591\u2592\u2593\u2592 \u2592 \u2592 \u2591\u2592\u2593\u2588\u2588\u2588\u2580\u2592\u2591\u2591 \u2592\u2591 \u2591\u2591 \u2592\u2593 \u2591\u2592\u2593\u2591\u2591\u2593 \u2592\u2592 \u2593\u2592\u2588\u2591 \u2591 \u2591\u2591\u2592\u2591 \u2591 \u2591 \u2592\u2591\u2592 \u2591 \u2591 \u2591 \u2591 \u2591\u2592 \u2591 \u2592\u2591 \u2592 \u2591 \u2592 \u2592\u2592 \u2591 \u2591 \u2591\u2591\u2591 \u2591 \u2591 \u2591 \u2591 \u2591 \u2591\u2591 \u2591 \u2592 \u2591 \u2591 \u2592 \u2591 \u2591 \u2591 \u2591 \u2591 \u2591 \u2591 \u2591 Version 0 .0.0 If you can see that funky logo your installation is correct. Note that the version may change. If you do not want to use poetry run in front of all your commands just activate the virtual environment with poetry shell . Use exit if you want to deactivate the environment.","title":"Getting started"},{"location":"contributing/#how-do-i-build-the-package","text":"You can build the package without installing the dependencies or without a proper Spark installation. Use make build or just make . You should see something like: $ make poetry build Building tuberia ( 0 .0.0 ) - Building sdist - Built tuberia-0.0.0.tar.gz - Building wheel - Built tuberia-0.0.0-py3-none-any.whl","title":"How do I build the package?"},{"location":"contributing/#how-do-i-run-tests","text":"Run tests locally with: $ make test","title":"How do I run tests?"},{"location":"contributing/#contribution-guidelines","text":"The code is auto-formatted by Black, so you can write the code without following any style guide and a Black pre-commit will take care of making it consistent with the current codebase. Write tests: test not added in the PR, test that will never be added.","title":"Contribution guidelines"},{"location":"faq/","text":"Frequently Asked Questions How does tuberia compare with dbt? dbt is more SQL-centric and handles queries as templates. In tuberia tables are classes that can execute arbitrary Python code (including SQL if for example you are using PySpark). To do unit tests in dbt you must learn how to define them and where to put them (you must define a macro that allows you to change where the table you want to test takes data from, you need to add a separate file with the test data...). In tuberia, if you know how to test in Python you know how to test the classes that create tables (mocks to the rescue!). In dbt dynamic queries can get complicated with the use of Jinja templates. When it comes to dynamic code there is nothing simpler than using a programming language to define transformations. Note Using arbitrary Python code in dbt is coming but it is still to early to compare. How does tuberia compare with Prefect? Tuberia is not an orchestrator. Tuberia can be seen more as a compiler, passing Python code that anyone with Python skills can handle, to a Prefect flow (or to any other orchestration tool) that you can deploy in your environment. How does tuberia compare with Airflow? Tuberia is not an orchestrator. Tuberia can be seen more as a compiler, passing Python code that anyone with Python skills can handle, to an Airflow DAG (or any other orchestration tool) that you can deploy in your environment.","title":"FAQ"},{"location":"faq/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"faq/#how-does-tuberia-compare-with-dbt","text":"dbt is more SQL-centric and handles queries as templates. In tuberia tables are classes that can execute arbitrary Python code (including SQL if for example you are using PySpark). To do unit tests in dbt you must learn how to define them and where to put them (you must define a macro that allows you to change where the table you want to test takes data from, you need to add a separate file with the test data...). In tuberia, if you know how to test in Python you know how to test the classes that create tables (mocks to the rescue!). In dbt dynamic queries can get complicated with the use of Jinja templates. When it comes to dynamic code there is nothing simpler than using a programming language to define transformations. Note Using arbitrary Python code in dbt is coming but it is still to early to compare.","title":"How does tuberia compare with dbt?"},{"location":"faq/#how-does-tuberia-compare-with-prefect","text":"Tuberia is not an orchestrator. Tuberia can be seen more as a compiler, passing Python code that anyone with Python skills can handle, to a Prefect flow (or to any other orchestration tool) that you can deploy in your environment.","title":"How does tuberia compare with Prefect?"},{"location":"faq/#how-does-tuberia-compare-with-airflow","text":"Tuberia is not an orchestrator. Tuberia can be seen more as a compiler, passing Python code that anyone with Python skills can handle, to an Airflow DAG (or any other orchestration tool) that you can deploy in your environment.","title":"How does tuberia compare with Airflow?"},{"location":"tags/","text":"Tags Following is a list of relevant tags:","title":"Tags"},{"location":"tags/#tags","text":"Following is a list of relevant tags:","title":"Tags"},{"location":"include/tep_header/","text":"","title":"Tep header"},{"location":"teps/tep00/","text":"TEP 0 - Tuberia Enhancement Proposals #tep-table td img { vertical-align: top; border-radius: 10px; } Authors: guiferviz Status: \u2705 Active Created: 2022-11-14 Tuberia version: 0.0.1 This first TEP introduces TEPs. Here we define what TEPs are, what process should be followed to propose one and what information they should contain. What is a TEP? TEP stands for Tuberia Enhancement Proposal. A TEP is a design document providing information to the Tuberia community, or describing a new feature for Tuberia or its processes or environment. TEP is to Tuberia what PEP is to Python. In fact, the first two sentences of this paragraph are an adaptation of the first lines of PEP 1 . Creation process At the moment, the creation process is very simple: Create a new file inside docs/teps . The number of the new TEP should be chosen by adding 1 to the last existing TEP. Copy the header macro of this TEP and update the different values. Learn how to set the status in the following flow diagram. Write the document. See more about this point in the following section. Publish a PR to start the review/discussion. Flow diagram that defines how to set an status: graph TD Q{Required implementation?} -->|No| A[Active] Q -->|Yes| P[Planned] P -->|when implemented| A P -->|when no longer valid| D[Deprecated] A -->|when no longer valid| D What a TEP should contain? TEP files should be written in a friendly way, without assuming too much knowledge on the part of the reader. It is a technical document, but technical terms should not be abused. A common structure is not required, just make sure it is comprehensible. Include an abstract after the metadata table at the beginning and organise the article into sections of your choice.","title":"TEP 0 - Tuberia Enhancement Proposals"},{"location":"teps/tep00/#tep-0-tuberia-enhancement-proposals","text":"#tep-table td img { vertical-align: top; border-radius: 10px; } Authors: guiferviz Status: \u2705 Active Created: 2022-11-14 Tuberia version: 0.0.1 This first TEP introduces TEPs. Here we define what TEPs are, what process should be followed to propose one and what information they should contain.","title":"TEP 0 - Tuberia Enhancement Proposals"},{"location":"teps/tep00/#what-is-a-tep","text":"TEP stands for Tuberia Enhancement Proposal. A TEP is a design document providing information to the Tuberia community, or describing a new feature for Tuberia or its processes or environment. TEP is to Tuberia what PEP is to Python. In fact, the first two sentences of this paragraph are an adaptation of the first lines of PEP 1 .","title":"What is a TEP?"},{"location":"teps/tep00/#creation-process","text":"At the moment, the creation process is very simple: Create a new file inside docs/teps . The number of the new TEP should be chosen by adding 1 to the last existing TEP. Copy the header macro of this TEP and update the different values. Learn how to set the status in the following flow diagram. Write the document. See more about this point in the following section. Publish a PR to start the review/discussion. Flow diagram that defines how to set an status: graph TD Q{Required implementation?} -->|No| A[Active] Q -->|Yes| P[Planned] P -->|when implemented| A P -->|when no longer valid| D[Deprecated] A -->|when no longer valid| D","title":"Creation process"},{"location":"teps/tep00/#what-a-tep-should-contain","text":"TEP files should be written in a friendly way, without assuming too much knowledge on the part of the reader. It is a technical document, but technical terms should not be abused. A common structure is not required, just make sure it is comprehensible. Include an abstract after the metadata table at the beginning and organise the article into sections of your choice.","title":"What a TEP should contain?"},{"location":"teps/tep01/","text":"TEP 1 - A pipeline compiler #tep-table td img { vertical-align: top; border-radius: 10px; } Authors: guiferviz Status: \ud83d\udcc5 Planned Created: 2022-12-08 Tuberia version: 0.0.1 We want Tuberia to be a pipeline compiler. A typical programming language compiler translates from a source code into a different source code (usually machine code, code that can be executed by the CPU of the platform that we are using). We can divide a compiler in different phases: graph TD src[/Source code/] analysis[Lexical/Syntax/Semantic analysis] ir[Intermediate code generation] optim[Code optimization] codegen[Code generation] code[/Source code/] src --> analysis analysis --> ir ir --> optim optim --> codegen codegen --> code Compiler phases In the case of Tuberia, the compiler phases would be similar, but instead of working with source code, it would work with Python objects representing pipelines. The compiler would have to analyze the objects and their relationships to build an intermediate representation, such as an AST or a DAG, and then generate code that can be executed by a specific orchestration tool, such as Airflow or Prefect. This code would define the steps and dependencies of the data pipeline, and would be executed by the chosen orchestration tool to run the data pipeline. In this TEP we will discuss the high-level design of the Tuberia compiler. In future TEPs we will elaborate on each of the compiler elements defined here. The goal of this TEP is to provide a high-level overview of the proposed design of the Tuberia compiler, and to spark discussion and feedback on the direction and potential of Tuberia. Elements of a pipeline compiler Like any other computer program, a pipeline compiler such as Tuberia must have inputs and outputs. The compilation process can also be divided into certain phases. So far these are the phases we have identified graph TD input[/\"Input (Python objects)\"/] compiler[Compiler] plugin[Plugins] output[/\"Output (Orchestrator source code)\"/] input --> compiler compiler --> output plugin --> compiler Tuberia key elements Each of these phases will be described in more detail below. Inputs The inputs of a classical code compiler are source code files. A code compiler reads the text in these files and converts it into an abstract syntax tree (AST). For example, an AST of 3 + 4 is: graph TD sum((+)) --> A((3)) sum --> B((4)) AST of 3 + 4 In the case of Tuberia we do not want to reinvent the wheel and design a new language, but we want to reuse the existing ones. Specifically Python, one of the most widespread languages in the world of data. The input to the compiler would be Python objects representing steps in the pipeline, rather than source code files. Python is a widely-used and well-known language, many developers are already familiar with it, which means they can start using Tuberia without a steep learning curve. Additionally, Python has a rich ecosystem of libraries and tools (pandas, pyspark, polars...) that can be used in conjunction with Tuberia to build powerful data pipelines. Tuberia will not create an AST itself, but it will create a DAG (Directed Acyclic Graph) in which each node will be an indivisible unit of work. For example, look at the following code: class Table0 : def create ( self ): # Create table 0 here. class Table1 : def create ( self ): # Create table 1 here. class JoinTables : table0 : Table0 table1 : Table1 def create ( self ): # Create a combined table using table 0 and table 1. The previous code can be depicted as follows: graph TD t0[Table0] t1[Table1] j[JoinTables] t0 --> j t1 --> j In this example each unit of work creates a table. JoinTables needs from Table0 and Table1, in other words, JoinTables depends on Table0 and Table1. The dependencies between each of the tables is described in the class definition. Table0 and Table1 do not contain any references to other classes, so they do not depend on any previous step. JoinTables do contain two references to other classes, so it is implicit that they depend on them to be created. This structure of interrelated objects will be the \"source code\" of Tuberia compiler. Compiler The Tuberia compiler is responsible for analyzing the input Python objects and generating intermediate code, optimizing the code, and finally generating orchestrator source code. This process is similar to that of a typical compiler, but it works with Python objects representing data pipelines instead of source code. The compiler uses plugins to support different orchestration tools, allowing users to generate code for the tool of their choice. A orchestrator target in Tuberia is the type of orchestrator for which the compiler will generate code. For example, a user could specify that they want to generate code for Airflow, in which case the compiler would use the appropriate plugin to generate the code in the correct format for Airflow. This allows users to choose the orchestrator that best fits their needs and use Tuberia to easily generate code for it. Apart from a target orchestrator we need to define the target executor. For example, if you are creating PySpark transformations that you want to run in Databricks from Airflow you need to differentiate between the target orchestrator (Airflow) and the target executor (Databricks). The target executor is the environment in which each of the steps of our pipeline will be executed, such as Databricks or a local Spark cluster. This information is used by the compiler to generate the orchestrator code that is compatible with the specified executor, allowing users to easily run their pipelines in the desired environment. To continue with the previous example, the generated Airflow DAG will make use of Databricks Operators if the execution target is Databricks. If the execution target is Python, a Python Operator could be used. Plugins Plugins are used in Tuberia to support different orchestrators and executors. Each plugin is responsible for generating code that is compatible with a specific orchestrator or executor. This allows users to easily generate code for the tool of their choice, without needing to worry about the specifics of the target platform. Plugins also make it easy for developers to add support for new orchestrators or executors to Tuberia, allowing the tool to be used with a wide range of platforms. In addition to generating code, plugins in Tuberia can also modify the intermediate representation of the pipeline (the DAG) to add additional functionality or features. For example, a plugin could wrap existing steps with logging or error handling. By allowing plugins to modify the intermediate representation, Tuberia can be extended and customized to support a wide range of functionality and use cases. A plugin could analyze the DAG and merge multiple tasks into a single task if it knows that doing so would be more efficient. By allowing plugins to optimize the generated code, Tuberia can help users to create more efficient and performant data pipelines. Outputs The outputs of Tuberia are source code files that can be executed by the chosen orchestrator. The exact format of these files will depend on the target orchestrator. For example, if the user has chosen to generate code for Airflow to run on a Databricks cluster, the output of Tuberia will be a Python file containing the Airflow DAG. On the other hand, if the target orchestrator was a Databricks Multi-Step job, a JSON file that you can use to send to the Databricks API will be generated. Tuberia is designed to generate code that can be manually deployed by the user ( or automated in the user CI/CD pipeline), but this behavior is not specified in this TEP. In general, deployment of generated code is a separate process from code generation and is not typically handled by Tuberia itself. We will probably introduce this feature in the future. Example Here is an example of how Tuberia might be used to compile a data pipeline in Python. First, we define some Python classes representing steps in our pipeline. For example: class ExtractData : def run ( self ): # Extract data from a database and save it to a staging table. class TransformData : extract_data : ExtractData def run ( self ): # Transform the data using a specific set of rules and save the output # into another staging table. class LoadData : transform_data : TransformData def run ( self ): # Create some KPIs and load the transformed data into a data warehouse. Next, we use Tuberia to compile these classes into code that can be executed by an orchestrator. We specify the target orchestrator and executor, such as Airflow and Databricks, and use the appropriate plugin to generate the code. This is the proposed syntax for doing so, although it is not yet implemented: # Define the target orchestrator and executor. orchestrator = \"airflow\" executor = \"databricks\" # Compile the pipeline using Tuberia. code = tuberia . compile ( LoadData , orchestrator , executor ) Once the Tuberia compiler has generated the code for the data pipeline, it is up to the user to decide what to do with it. The generated code can be deployed to the target orchestrator and executed to run the pipeline, or it can be modified or extended by the user as needed. It is important to note that the generated code may not always work, and the user may need to make additional changes or adjustments in his/her infrastructure to get it working as desired. For example, if the generated code is using an Airflow operator that is not installed in your Airflow environment you will get error if you try to execute the generated code. Advanced usage In addition to being able to generate code for the pipeline, having access to the DAG representation of the pipeline can also provide other advantages. For example, you can use the DAG to generate documentation for the pipeline, which can be useful for other members of your team who need to understand the pipeline's structure and dependencies. Additionally, the DAG can be used to generate extra steps in your pipelines , such as checking the schema of the data being processed match the expected one or checking the quality of the data to ensure that it meets certain criteria. By leveraging the DAG representation of the pipeline, Tuberia can help users to create more robust and well-documented data pipelines. Conclusion Overall, the design of a data pipeline using Tuberia would involve creating Python objects representing pipeline steps, using a compiler to generate code or configuration files for the target orchestrator. This design would allow users to easily define and deploy data pipelines using Python objects, without having to manually write code or configuration files for the target orchestration platform. This TEP is an initial proposal for the design of the Tuberia compiler, and it is subject to change and modification. The ideas and concepts presented in this TEP are intended to serve as a starting point for discussion and further development, and are not intended to be definitive or final. As such, it is possible that some of the ideas and proposals in this TEP may be refined, expanded upon, or even removed in future TEPs.","title":"TEP 1 - A pipeline compiler"},{"location":"teps/tep01/#tep-1-a-pipeline-compiler","text":"#tep-table td img { vertical-align: top; border-radius: 10px; } Authors: guiferviz Status: \ud83d\udcc5 Planned Created: 2022-12-08 Tuberia version: 0.0.1 We want Tuberia to be a pipeline compiler. A typical programming language compiler translates from a source code into a different source code (usually machine code, code that can be executed by the CPU of the platform that we are using). We can divide a compiler in different phases: graph TD src[/Source code/] analysis[Lexical/Syntax/Semantic analysis] ir[Intermediate code generation] optim[Code optimization] codegen[Code generation] code[/Source code/] src --> analysis analysis --> ir ir --> optim optim --> codegen codegen --> code Compiler phases In the case of Tuberia, the compiler phases would be similar, but instead of working with source code, it would work with Python objects representing pipelines. The compiler would have to analyze the objects and their relationships to build an intermediate representation, such as an AST or a DAG, and then generate code that can be executed by a specific orchestration tool, such as Airflow or Prefect. This code would define the steps and dependencies of the data pipeline, and would be executed by the chosen orchestration tool to run the data pipeline. In this TEP we will discuss the high-level design of the Tuberia compiler. In future TEPs we will elaborate on each of the compiler elements defined here. The goal of this TEP is to provide a high-level overview of the proposed design of the Tuberia compiler, and to spark discussion and feedback on the direction and potential of Tuberia.","title":"TEP 1 - A pipeline compiler"},{"location":"teps/tep01/#elements-of-a-pipeline-compiler","text":"Like any other computer program, a pipeline compiler such as Tuberia must have inputs and outputs. The compilation process can also be divided into certain phases. So far these are the phases we have identified graph TD input[/\"Input (Python objects)\"/] compiler[Compiler] plugin[Plugins] output[/\"Output (Orchestrator source code)\"/] input --> compiler compiler --> output plugin --> compiler Tuberia key elements Each of these phases will be described in more detail below.","title":"Elements of a pipeline compiler"},{"location":"teps/tep01/#inputs","text":"The inputs of a classical code compiler are source code files. A code compiler reads the text in these files and converts it into an abstract syntax tree (AST). For example, an AST of 3 + 4 is: graph TD sum((+)) --> A((3)) sum --> B((4)) AST of 3 + 4 In the case of Tuberia we do not want to reinvent the wheel and design a new language, but we want to reuse the existing ones. Specifically Python, one of the most widespread languages in the world of data. The input to the compiler would be Python objects representing steps in the pipeline, rather than source code files. Python is a widely-used and well-known language, many developers are already familiar with it, which means they can start using Tuberia without a steep learning curve. Additionally, Python has a rich ecosystem of libraries and tools (pandas, pyspark, polars...) that can be used in conjunction with Tuberia to build powerful data pipelines. Tuberia will not create an AST itself, but it will create a DAG (Directed Acyclic Graph) in which each node will be an indivisible unit of work. For example, look at the following code: class Table0 : def create ( self ): # Create table 0 here. class Table1 : def create ( self ): # Create table 1 here. class JoinTables : table0 : Table0 table1 : Table1 def create ( self ): # Create a combined table using table 0 and table 1. The previous code can be depicted as follows: graph TD t0[Table0] t1[Table1] j[JoinTables] t0 --> j t1 --> j In this example each unit of work creates a table. JoinTables needs from Table0 and Table1, in other words, JoinTables depends on Table0 and Table1. The dependencies between each of the tables is described in the class definition. Table0 and Table1 do not contain any references to other classes, so they do not depend on any previous step. JoinTables do contain two references to other classes, so it is implicit that they depend on them to be created. This structure of interrelated objects will be the \"source code\" of Tuberia compiler.","title":"Inputs"},{"location":"teps/tep01/#compiler","text":"The Tuberia compiler is responsible for analyzing the input Python objects and generating intermediate code, optimizing the code, and finally generating orchestrator source code. This process is similar to that of a typical compiler, but it works with Python objects representing data pipelines instead of source code. The compiler uses plugins to support different orchestration tools, allowing users to generate code for the tool of their choice. A orchestrator target in Tuberia is the type of orchestrator for which the compiler will generate code. For example, a user could specify that they want to generate code for Airflow, in which case the compiler would use the appropriate plugin to generate the code in the correct format for Airflow. This allows users to choose the orchestrator that best fits their needs and use Tuberia to easily generate code for it. Apart from a target orchestrator we need to define the target executor. For example, if you are creating PySpark transformations that you want to run in Databricks from Airflow you need to differentiate between the target orchestrator (Airflow) and the target executor (Databricks). The target executor is the environment in which each of the steps of our pipeline will be executed, such as Databricks or a local Spark cluster. This information is used by the compiler to generate the orchestrator code that is compatible with the specified executor, allowing users to easily run their pipelines in the desired environment. To continue with the previous example, the generated Airflow DAG will make use of Databricks Operators if the execution target is Databricks. If the execution target is Python, a Python Operator could be used.","title":"Compiler"},{"location":"teps/tep01/#plugins","text":"Plugins are used in Tuberia to support different orchestrators and executors. Each plugin is responsible for generating code that is compatible with a specific orchestrator or executor. This allows users to easily generate code for the tool of their choice, without needing to worry about the specifics of the target platform. Plugins also make it easy for developers to add support for new orchestrators or executors to Tuberia, allowing the tool to be used with a wide range of platforms. In addition to generating code, plugins in Tuberia can also modify the intermediate representation of the pipeline (the DAG) to add additional functionality or features. For example, a plugin could wrap existing steps with logging or error handling. By allowing plugins to modify the intermediate representation, Tuberia can be extended and customized to support a wide range of functionality and use cases. A plugin could analyze the DAG and merge multiple tasks into a single task if it knows that doing so would be more efficient. By allowing plugins to optimize the generated code, Tuberia can help users to create more efficient and performant data pipelines.","title":"Plugins"},{"location":"teps/tep01/#outputs","text":"The outputs of Tuberia are source code files that can be executed by the chosen orchestrator. The exact format of these files will depend on the target orchestrator. For example, if the user has chosen to generate code for Airflow to run on a Databricks cluster, the output of Tuberia will be a Python file containing the Airflow DAG. On the other hand, if the target orchestrator was a Databricks Multi-Step job, a JSON file that you can use to send to the Databricks API will be generated. Tuberia is designed to generate code that can be manually deployed by the user ( or automated in the user CI/CD pipeline), but this behavior is not specified in this TEP. In general, deployment of generated code is a separate process from code generation and is not typically handled by Tuberia itself. We will probably introduce this feature in the future.","title":"Outputs"},{"location":"teps/tep01/#example","text":"Here is an example of how Tuberia might be used to compile a data pipeline in Python. First, we define some Python classes representing steps in our pipeline. For example: class ExtractData : def run ( self ): # Extract data from a database and save it to a staging table. class TransformData : extract_data : ExtractData def run ( self ): # Transform the data using a specific set of rules and save the output # into another staging table. class LoadData : transform_data : TransformData def run ( self ): # Create some KPIs and load the transformed data into a data warehouse. Next, we use Tuberia to compile these classes into code that can be executed by an orchestrator. We specify the target orchestrator and executor, such as Airflow and Databricks, and use the appropriate plugin to generate the code. This is the proposed syntax for doing so, although it is not yet implemented: # Define the target orchestrator and executor. orchestrator = \"airflow\" executor = \"databricks\" # Compile the pipeline using Tuberia. code = tuberia . compile ( LoadData , orchestrator , executor ) Once the Tuberia compiler has generated the code for the data pipeline, it is up to the user to decide what to do with it. The generated code can be deployed to the target orchestrator and executed to run the pipeline, or it can be modified or extended by the user as needed. It is important to note that the generated code may not always work, and the user may need to make additional changes or adjustments in his/her infrastructure to get it working as desired. For example, if the generated code is using an Airflow operator that is not installed in your Airflow environment you will get error if you try to execute the generated code.","title":"Example"},{"location":"teps/tep01/#advanced-usage","text":"In addition to being able to generate code for the pipeline, having access to the DAG representation of the pipeline can also provide other advantages. For example, you can use the DAG to generate documentation for the pipeline, which can be useful for other members of your team who need to understand the pipeline's structure and dependencies. Additionally, the DAG can be used to generate extra steps in your pipelines , such as checking the schema of the data being processed match the expected one or checking the quality of the data to ensure that it meets certain criteria. By leveraging the DAG representation of the pipeline, Tuberia can help users to create more robust and well-documented data pipelines.","title":"Advanced usage"},{"location":"teps/tep01/#conclusion","text":"Overall, the design of a data pipeline using Tuberia would involve creating Python objects representing pipeline steps, using a compiler to generate code or configuration files for the target orchestrator. This design would allow users to easily define and deploy data pipelines using Python objects, without having to manually write code or configuration files for the target orchestration platform. This TEP is an initial proposal for the design of the Tuberia compiler, and it is subject to change and modification. The ideas and concepts presented in this TEP are intended to serve as a starting point for discussion and further development, and are not intended to be definitive or final. As such, it is possible that some of the ideas and proposals in this TEP may be refined, expanded upon, or even removed in future TEPs.","title":"Conclusion"},{"location":"teps/tep02/","text":"TEP 2 - Flexible and simple tasks #tep-table td img { vertical-align: top; border-radius: 10px; } Authors: guiferviz Status: \ud83d\udcc5 Planned Created: 2022-11-14 Tuberia version: 0.0.1 The equivalent of source code for Tuberia is Python objects representing data pipeline tasks and their dependencies. In Tuberia, users define their data pipelines using Python classes and methods, rather than using a specific language or syntax. This allows users to leverage their existing knowledge of Python and its ecosystem of libraries and tools to create powerful and flexible data pipelines. In this TEP, we will discuss how to create tasks and specify dependencies between them in Tuberia. We will describe the different types of tasks that can be defined, and provide examples of how to write and use these tasks in your data pipelines will look like. Name selection: Tasks or Steps? In the context of data pipelines, the terms step and task are often used interchangeably to refer to an indivisible unit of work. Both terms are used to describe a specific action or operation that is performed as part of a data pipeline, such as extracting data from a database, transforming the data, or loading the data into a target system. However, there may be some subtle differences in the way that these terms are used. For example, the term \"step\" may be used to refer to a specific operation or action that is performed in a linear, sequential manner, as part of a larger process. On the other hand, the term \"task\" may be used to refer to a standalone operation or action that can be performed independently, and may not necessarily be part of a larger process. I personally like the definitions provided by this page . Quote A step is part of a group. A task is alone. A step wants to feel like it belongs to a bigger purpose, contributing to others. A task is selfish, only thinking of itself. Overall, the difference between the terms \"step\" and \"task\" may be somewhat subtle, and may depend on the context in which they are used. Which term will be using Tuberia then? In the humble opinion of the author of these lines, the word Task seems to be the most common word in computer science. Libraries such as Prefect already define the concept of tasks. There are hundreds of libraries that sell themselves as task runners. It is not so common to find \"step runners\", for example. Due to the popularity of the word Task and the subtle differences it has with Step, Task is chosen as the name for the indivisible units of work in Tuberia. Existing libraries One design decision that was made in the development of Tuberia was to use Python classes to represent the tasks or steps of a data pipeline. This decision was based on the fact that Python is a widely-used and well-known language, and many developers are already familiar with it. By using Python classes to represent the tasks of a data pipeline, Tuberia can leverage the existing knowledge and expertise of developers, and make it easy for them to start using Tuberia without a steep learning curve. Another potential design decision was to use an existing library, such as Prefect, to create tasks or steps in the data pipeline. Prefect is a popular Python library for creating and managing data pipelines, and using it to create tasks in Tuberia could have potentially saved time and effort in the development of the compiler. Prefect is the library for creating tasks which I am most familiar with. Here is an example of how to create an use a task. In this example we are also creating a prefect Flow, equivalent to the dependency tree that we also want to define in this TEP. from prefect import Task class AddTask ( Task ): def run ( self , x , y ): return x + y a = AddTask () with Flow ( \"My Flow\" ) as f : t1 = a ( 1 , 2 ) # t1 != a t2 = a ( 5 , 7 ) # t2 != a The least convincing part of this implementation is that the parameters that define the execution are passed to the run method. You can create an __init__ method in your Task subclass but the parameters you pass to it must be any other type of data than Task objects. It's quite confusing to have 2 different ways to pass parameters to your task. I would prefer all parameters in the __init__ method. Prefect also comes with decorators. from prefect import task @task def add_task ( x , y ): return x + y For simple tasks this may be fine, but most of the time we will have a lot of parameters. Think about PySpark table creation; we must have the database name, the table name, the input tables, the data expectations we must apply, the table schema... We can subclass from Task and create a PySparkTable class with all those common table parameters and then create a decorator that creates tables using the PySparkTable class. Pseudocode: from prefect import task class PySparkTable ( Task ): ... # Define decorator def pyspark_table ( ... ): ... @pyspark_table ( database_name = \"my_database\" , table_name = \"my_table\" , data_expectations =... schema =... ) def join_tables ( table0 , table1 ): # Create table from tables table0 and table1. ... Again, same problem as before, the parameters passed to the decorator are indeed passed to the __init__ method. The function parameters are run method parameters. Task dependencies cannot be passed to __init__ , just to run . Apart from that, it is not possible to get the database_name or table_name from the function body, which make this approach difficult to use. There is one observation more, imagine that we have two functions table0 and table1 decorated with our pyspark_table decorator. We need to save those tables in a variable in order to pass them to the join_tables task: @pyspark_table def table0 (): ... @pyspark_table def table1 (): ... @pyspark_table def join_tables ( table0 , table1 ): ... with Flow ( \"My Flow\" ) as f : table0 = table0 () table1 = table1 () join_tables = join_tables ( table0 , table1 ) Do you see any problem in the previous code? We are naming our functions using the name of the tables. It makes sense to create variables with exactly the same names, but it is a problem as we are overwriting the functions. In this example it is not clear if we are passing table0 and table1 functions to our join_tables or if we are passing the task objects. There are more issues with this approach. Just looking at join_tables , what can we say about table0 ? Can we use any PySpark table here or it should have a concrete schema? Using decorators we loose type annotations. If we create classes we have a type that we can use to annotate parameters. Besides that, we can easily name the variables. For example: class Table0 ( PySparkTable ): ... class Table1 ( PySparkTable ): ... class JoinTables ( PySparkTable ): table0 : Table0 table1 : Table1 def __init__ ( self , table0 : Table0 , table1 : Table1 ): self . table0 = table0 self . table1 = table1 def run ( self ): ... The previous code does not work in Prefect because we are using tasks in our __init__ method but we can see that this approach provides typing annotations and avoids name collisions ( Table0 can now be assigned to table0 without hiding any function). Note On the other hand, Prefect requires a lot of dependencies, so if it can be avoided, the better. I could not find more libraries following an approach similar to what I have in mind. I did explore invoke but it is more related to make than to Prefect. celery deals with distributed tasks. Dependencies In this section we will discuss two different approaches to define dependencies between tasks. One approach is using a get_dependencies method in our Task objects. Another approach is to use a dependency manager that extracts the dependencies of a pipeline from the object attribute. Manually define dependencies In this approach, each Task object defines a get_dependencies method that returns a list of tasks that the current task depends on. This method can be overridden by subclasses to define the specific dependencies of each task. For example: class Task : def __init__ ( self ): self . dependencies = [] def get_dependencies ( self ): return self . dependencies class ExtractData ( Task ): def __init__ ( self ): super () . __init__ () self . dependencies = [] class TransformData ( Task ): def __init__ ( self ): super () . __init__ () self . dependencies = [ ExtractData ] class LoadData ( Task ): def __init__ ( self ): super () . __init__ () self . dependencies = [ TransformData ] In this example, TransformData depends on ExtractData , and LoadData depends on TransformData . This approach allows us to define the dependencies of each task in a clear and concise way, and makes it easy to modify or update the dependencies as needed. However, this approach has a couple of drawbacks. If the user does not implement the get_dependencies method correctly (because he/she forgets about adding the object to the dependencies list, for example), the dependencies for the Task object may not be extracted correctly, and this could lead to errors in the generated DAG for the data pipeline. Another potential problem is that this approach requires the user to include a get_dependencies method on every Task object that has dependencies. This can be a significant amount of extra code for the user to write and maintain, especially if the user has many Task objects in their data pipeline. This extra code can make the data pipeline compiler more difficult to use and understand, and it may decrease its overall usability. Overall, while this approach allows users to define custom get_dependencies methods on their Step objects, it may introduce additional complexity and potential errors in the data pipeline compiler. A simpler approach, such as automatically extracting dependencies from the attributes of the Task objects, may be more suitable in some cases. Automatically extracting dependencies The second approach is to use a dependency manager to extract the dependencies of a Task from the object attributes. This means that instead of defining a list of dependencies in the Task object itself, the dependency manager would inspect the attributes of the object and extract the dependencies from there. For example, consider the following code: class ExtractData : def run ( self ): # Extract data here. class TransformData : extract_data : ExtractData def run ( self ): # Transform data here using the extracted data. In this example, the TransformData class depends on the ExtractData class. A dependency manager could inspect the extract_data attribute of the TransformData class and determine that TransformData depends on ExtractData . This approach allows the user to define dependencies in a more natural and intuitive way, by simply setting the attributes of the Task objects. However, this approach also has some drawbacks. For example, it may not always be clear which attributes of a Task object represent dependencies, and it may be difficult to ensure that all dependencies are properly defined (specially when using attributes with data structures like dicts or list that contain Tasks). Additionally, this approach may not be as flexible as the get_dependencies method, as it may be difficult to define complex or dynamic dependencies using object attributes. Hybrid approach One potential solution to the limitations of the two approaches discussed above is to use a hybrid approach that combines the best features of both. In this approach, a Task object could define a get_dependencies method if it needs to define complex or dynamic dependencies, and the dependency manager would use this method to extract the dependencies. If the get_dependencies method is not defined, the dependency manager would fall back to inspecting the object attributes to extract the dependencies. This hybrid approach would allow Task objects to define complex or dynamic dependencies using the get_dependencies method, while still allowing for simple and intuitive definitions of dependencies using object attributes. Here is an example of how this hybrid approach could be implemented: class ExtractData : def run ( self ): # Extract data here. class TransformData : extract_data : ExtractData def run ( self ): # Transform data here using the extracted data. def get_dependencies ( self ): return [ self . extract_data ] In this example, the TransformData class defines a get_dependencies method that returns a list of dependencies. The dependency manager would use this method to extract the dependencies of the TransformData class. If the get_dependencies method was not defined, the dependency manager would fall back to inspecting the extract_data attribute of the TransformData class to determine the dependencies. Task properties A class representing a task in a data pipeline should have certain properties to facilitate the creation and management of the pipeline. These properties can include an ID, a name, tags... The ID property can be used to uniquely identify a task within a data pipeline. This is important because it allows the dependency manager to track and manage dependencies between tasks, and it also allows the user to refer to specific tasks in the pipeline if needed. The ID can be automatically generated by the dependency manager, or it can be explicitly set by the user. If we automatically generate IDs we need to be sure that the ID is consistent between runs. The name property can be used to provide a human-readable name for a task. This can be useful for documentation and debugging purposes, as it can help the user understand the purpose of a task and its place in the pipeline. The name property can be automatically derived from the class name, or it can be explicitly set by the user. Tags can be a useful property of tasks in a data pipeline. Tags can be used to group tasks by categories, such as by type, purpose, or any other relevant criteria. For example, tasks that are part of the same data transformation or data quality checking process can be grouped under the same tag. This can help the user understand the organization and structure of the pipeline, and it can also be useful for debugging and optimization purposes. Tags can also be used to group the execution of tags, i.e. executing multiple task in just one step of the orchestrator.","title":"TEP 2 - Flexible and simple tasks"},{"location":"teps/tep02/#tep-2-flexible-and-simple-tasks","text":"#tep-table td img { vertical-align: top; border-radius: 10px; } Authors: guiferviz Status: \ud83d\udcc5 Planned Created: 2022-11-14 Tuberia version: 0.0.1 The equivalent of source code for Tuberia is Python objects representing data pipeline tasks and their dependencies. In Tuberia, users define their data pipelines using Python classes and methods, rather than using a specific language or syntax. This allows users to leverage their existing knowledge of Python and its ecosystem of libraries and tools to create powerful and flexible data pipelines. In this TEP, we will discuss how to create tasks and specify dependencies between them in Tuberia. We will describe the different types of tasks that can be defined, and provide examples of how to write and use these tasks in your data pipelines will look like.","title":"TEP 2 - Flexible and simple tasks"},{"location":"teps/tep02/#name-selection-tasks-or-steps","text":"In the context of data pipelines, the terms step and task are often used interchangeably to refer to an indivisible unit of work. Both terms are used to describe a specific action or operation that is performed as part of a data pipeline, such as extracting data from a database, transforming the data, or loading the data into a target system. However, there may be some subtle differences in the way that these terms are used. For example, the term \"step\" may be used to refer to a specific operation or action that is performed in a linear, sequential manner, as part of a larger process. On the other hand, the term \"task\" may be used to refer to a standalone operation or action that can be performed independently, and may not necessarily be part of a larger process. I personally like the definitions provided by this page . Quote A step is part of a group. A task is alone. A step wants to feel like it belongs to a bigger purpose, contributing to others. A task is selfish, only thinking of itself. Overall, the difference between the terms \"step\" and \"task\" may be somewhat subtle, and may depend on the context in which they are used. Which term will be using Tuberia then? In the humble opinion of the author of these lines, the word Task seems to be the most common word in computer science. Libraries such as Prefect already define the concept of tasks. There are hundreds of libraries that sell themselves as task runners. It is not so common to find \"step runners\", for example. Due to the popularity of the word Task and the subtle differences it has with Step, Task is chosen as the name for the indivisible units of work in Tuberia.","title":"Name selection: Tasks or Steps?"},{"location":"teps/tep02/#existing-libraries","text":"One design decision that was made in the development of Tuberia was to use Python classes to represent the tasks or steps of a data pipeline. This decision was based on the fact that Python is a widely-used and well-known language, and many developers are already familiar with it. By using Python classes to represent the tasks of a data pipeline, Tuberia can leverage the existing knowledge and expertise of developers, and make it easy for them to start using Tuberia without a steep learning curve. Another potential design decision was to use an existing library, such as Prefect, to create tasks or steps in the data pipeline. Prefect is a popular Python library for creating and managing data pipelines, and using it to create tasks in Tuberia could have potentially saved time and effort in the development of the compiler. Prefect is the library for creating tasks which I am most familiar with. Here is an example of how to create an use a task. In this example we are also creating a prefect Flow, equivalent to the dependency tree that we also want to define in this TEP. from prefect import Task class AddTask ( Task ): def run ( self , x , y ): return x + y a = AddTask () with Flow ( \"My Flow\" ) as f : t1 = a ( 1 , 2 ) # t1 != a t2 = a ( 5 , 7 ) # t2 != a The least convincing part of this implementation is that the parameters that define the execution are passed to the run method. You can create an __init__ method in your Task subclass but the parameters you pass to it must be any other type of data than Task objects. It's quite confusing to have 2 different ways to pass parameters to your task. I would prefer all parameters in the __init__ method. Prefect also comes with decorators. from prefect import task @task def add_task ( x , y ): return x + y For simple tasks this may be fine, but most of the time we will have a lot of parameters. Think about PySpark table creation; we must have the database name, the table name, the input tables, the data expectations we must apply, the table schema... We can subclass from Task and create a PySparkTable class with all those common table parameters and then create a decorator that creates tables using the PySparkTable class. Pseudocode: from prefect import task class PySparkTable ( Task ): ... # Define decorator def pyspark_table ( ... ): ... @pyspark_table ( database_name = \"my_database\" , table_name = \"my_table\" , data_expectations =... schema =... ) def join_tables ( table0 , table1 ): # Create table from tables table0 and table1. ... Again, same problem as before, the parameters passed to the decorator are indeed passed to the __init__ method. The function parameters are run method parameters. Task dependencies cannot be passed to __init__ , just to run . Apart from that, it is not possible to get the database_name or table_name from the function body, which make this approach difficult to use. There is one observation more, imagine that we have two functions table0 and table1 decorated with our pyspark_table decorator. We need to save those tables in a variable in order to pass them to the join_tables task: @pyspark_table def table0 (): ... @pyspark_table def table1 (): ... @pyspark_table def join_tables ( table0 , table1 ): ... with Flow ( \"My Flow\" ) as f : table0 = table0 () table1 = table1 () join_tables = join_tables ( table0 , table1 ) Do you see any problem in the previous code? We are naming our functions using the name of the tables. It makes sense to create variables with exactly the same names, but it is a problem as we are overwriting the functions. In this example it is not clear if we are passing table0 and table1 functions to our join_tables or if we are passing the task objects. There are more issues with this approach. Just looking at join_tables , what can we say about table0 ? Can we use any PySpark table here or it should have a concrete schema? Using decorators we loose type annotations. If we create classes we have a type that we can use to annotate parameters. Besides that, we can easily name the variables. For example: class Table0 ( PySparkTable ): ... class Table1 ( PySparkTable ): ... class JoinTables ( PySparkTable ): table0 : Table0 table1 : Table1 def __init__ ( self , table0 : Table0 , table1 : Table1 ): self . table0 = table0 self . table1 = table1 def run ( self ): ... The previous code does not work in Prefect because we are using tasks in our __init__ method but we can see that this approach provides typing annotations and avoids name collisions ( Table0 can now be assigned to table0 without hiding any function). Note On the other hand, Prefect requires a lot of dependencies, so if it can be avoided, the better. I could not find more libraries following an approach similar to what I have in mind. I did explore invoke but it is more related to make than to Prefect. celery deals with distributed tasks.","title":"Existing libraries"},{"location":"teps/tep02/#dependencies","text":"In this section we will discuss two different approaches to define dependencies between tasks. One approach is using a get_dependencies method in our Task objects. Another approach is to use a dependency manager that extracts the dependencies of a pipeline from the object attribute.","title":"Dependencies"},{"location":"teps/tep02/#manually-define-dependencies","text":"In this approach, each Task object defines a get_dependencies method that returns a list of tasks that the current task depends on. This method can be overridden by subclasses to define the specific dependencies of each task. For example: class Task : def __init__ ( self ): self . dependencies = [] def get_dependencies ( self ): return self . dependencies class ExtractData ( Task ): def __init__ ( self ): super () . __init__ () self . dependencies = [] class TransformData ( Task ): def __init__ ( self ): super () . __init__ () self . dependencies = [ ExtractData ] class LoadData ( Task ): def __init__ ( self ): super () . __init__ () self . dependencies = [ TransformData ] In this example, TransformData depends on ExtractData , and LoadData depends on TransformData . This approach allows us to define the dependencies of each task in a clear and concise way, and makes it easy to modify or update the dependencies as needed. However, this approach has a couple of drawbacks. If the user does not implement the get_dependencies method correctly (because he/she forgets about adding the object to the dependencies list, for example), the dependencies for the Task object may not be extracted correctly, and this could lead to errors in the generated DAG for the data pipeline. Another potential problem is that this approach requires the user to include a get_dependencies method on every Task object that has dependencies. This can be a significant amount of extra code for the user to write and maintain, especially if the user has many Task objects in their data pipeline. This extra code can make the data pipeline compiler more difficult to use and understand, and it may decrease its overall usability. Overall, while this approach allows users to define custom get_dependencies methods on their Step objects, it may introduce additional complexity and potential errors in the data pipeline compiler. A simpler approach, such as automatically extracting dependencies from the attributes of the Task objects, may be more suitable in some cases.","title":"Manually define dependencies"},{"location":"teps/tep02/#automatically-extracting-dependencies","text":"The second approach is to use a dependency manager to extract the dependencies of a Task from the object attributes. This means that instead of defining a list of dependencies in the Task object itself, the dependency manager would inspect the attributes of the object and extract the dependencies from there. For example, consider the following code: class ExtractData : def run ( self ): # Extract data here. class TransformData : extract_data : ExtractData def run ( self ): # Transform data here using the extracted data. In this example, the TransformData class depends on the ExtractData class. A dependency manager could inspect the extract_data attribute of the TransformData class and determine that TransformData depends on ExtractData . This approach allows the user to define dependencies in a more natural and intuitive way, by simply setting the attributes of the Task objects. However, this approach also has some drawbacks. For example, it may not always be clear which attributes of a Task object represent dependencies, and it may be difficult to ensure that all dependencies are properly defined (specially when using attributes with data structures like dicts or list that contain Tasks). Additionally, this approach may not be as flexible as the get_dependencies method, as it may be difficult to define complex or dynamic dependencies using object attributes.","title":"Automatically extracting dependencies"},{"location":"teps/tep02/#hybrid-approach","text":"One potential solution to the limitations of the two approaches discussed above is to use a hybrid approach that combines the best features of both. In this approach, a Task object could define a get_dependencies method if it needs to define complex or dynamic dependencies, and the dependency manager would use this method to extract the dependencies. If the get_dependencies method is not defined, the dependency manager would fall back to inspecting the object attributes to extract the dependencies. This hybrid approach would allow Task objects to define complex or dynamic dependencies using the get_dependencies method, while still allowing for simple and intuitive definitions of dependencies using object attributes. Here is an example of how this hybrid approach could be implemented: class ExtractData : def run ( self ): # Extract data here. class TransformData : extract_data : ExtractData def run ( self ): # Transform data here using the extracted data. def get_dependencies ( self ): return [ self . extract_data ] In this example, the TransformData class defines a get_dependencies method that returns a list of dependencies. The dependency manager would use this method to extract the dependencies of the TransformData class. If the get_dependencies method was not defined, the dependency manager would fall back to inspecting the extract_data attribute of the TransformData class to determine the dependencies.","title":"Hybrid approach"},{"location":"teps/tep02/#task-properties","text":"A class representing a task in a data pipeline should have certain properties to facilitate the creation and management of the pipeline. These properties can include an ID, a name, tags... The ID property can be used to uniquely identify a task within a data pipeline. This is important because it allows the dependency manager to track and manage dependencies between tasks, and it also allows the user to refer to specific tasks in the pipeline if needed. The ID can be automatically generated by the dependency manager, or it can be explicitly set by the user. If we automatically generate IDs we need to be sure that the ID is consistent between runs. The name property can be used to provide a human-readable name for a task. This can be useful for documentation and debugging purposes, as it can help the user understand the purpose of a task and its place in the pipeline. The name property can be automatically derived from the class name, or it can be explicitly set by the user. Tags can be a useful property of tasks in a data pipeline. Tags can be used to group tasks by categories, such as by type, purpose, or any other relevant criteria. For example, tasks that are part of the same data transformation or data quality checking process can be grouped under the same tag. This can help the user understand the organization and structure of the pipeline, and it can also be useful for debugging and optimization purposes. Tags can also be used to group the execution of tags, i.e. executing multiple task in just one step of the orchestrator.","title":"Task properties"},{"location":"tags/","text":"Tags Following is a list of relevant tags:","title":"Tags"},{"location":"tags/#tags","text":"Following is a list of relevant tags:","title":"Tags"}]}